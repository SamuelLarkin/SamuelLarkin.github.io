@inproceedings{littell-etal-2018-measuring,
 abstract = {The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to score sentence pairs from a large high-recall, low-precision web-scraped parallel corpus (Koehn et al., 2018a). Participants could use existing sample corpora (e.g. past WMT data) as a supervisory signal to learn what a ``cleanâ€³ corpus looks like. However, in lower-resource situations it often happens that the target corpus of the language is the \textitonly sample of parallel text in that language. We therefore made several unsupervised entries, setting ourselves an additional constraint that we not utilize the additional clean parallel corpora. One such entry fairly consistently scored in the top ten systems in the 100M-word conditions, and for one task---translating the European Medicines Agency corpus (Tiedemann, 2009)---scored among the best systems even in the 10M-word conditions.},
 address = {Belgium, Brussels},
 author = {Littell, Patrick  and Larkin, Samuel  and Stewart, Darlene  and Simard, Michel  and Goutte, Cyril  and Lo, Chi-kiu},
 booktitle = {Proceedings of the Third Conference on Machine Translation: Shared Task Papers},
 doi = {10.18653/v1/W18-6480},
 month = {October},
 pages = {900--907},
 publisher = {Association for Computational Linguistics},
 title = {Measuring sentence parallelism using Mahalanobis distances: The NRC unsupervised submissions to the WMT18 Parallel Corpus Filtering shared task},
 url = {https://aclanthology.org/W18-6480},
 year = {2018}
}
