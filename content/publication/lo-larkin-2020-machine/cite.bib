@inproceedings{lo-larkin-2020-machine,
 abstract = {We present a study on using YiSi-2 with massive multilingual pretrained language models for machine translation (MT) reference-less evaluation. Aiming at finding better semantic representation for semantic MT evaluation, we first test YiSi-2 with contextual embed- dings extracted from different layers of two different pretrained models, multilingual BERT and XLM-RoBERTa. We also experiment with learning bilingual mappings that trans- form the vector subspace of the source language to be closer to that of the target language in the pretrained model to obtain more accurate cross-lingual semantic similarity representations. Our results show that YiSi-2â€²s correlation with human direct assessment on translation quality is greatly improved by replacing multilingual BERT with XLM-RoBERTa and projecting the source embeddings into the tar- get embedding space using a cross-lingual lin- ear projection (CLP) matrix learnt from a small development set.},
 address = {Online},
 author = {Lo, Chi-kiu  and Larkin, Samuel},
 booktitle = {Proceedings of the Fifth Conference on Machine Translation},
 month = {November},
 pages = {903--910},
 publisher = {Association for Computational Linguistics},
 title = {Machine Translation Reference-less Evaluation using YiSi-2 with Bilingual Mappings of Massive Multilingual Language Model},
 url = {https://aclanthology.org/2020.wmt-1.100},
 year = {2020}
}
